{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a570a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import ast\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "class CustomPipeline:\n",
    "    def __init__(self, max_length=128, min_freq=2):\n",
    "        self.max_length = max_length\n",
    "        self.min_freq = min_freq\n",
    "        self.vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.word_counts = Counter()\n",
    "        \n",
    "    def tokenizer(self, text):\n",
    "        # Simple whitespace and punctuation split\n",
    "        text = text.lower()\n",
    "        # Keep words and basic punctuation\n",
    "        tokens = re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        print(\"Building Vocabulary...\")\n",
    "        for text in texts:\n",
    "            tokens = self.tokenizer(text)\n",
    "            self.word_counts.update(tokens)\n",
    "            \n",
    "        # Only keep words that appear at least min_freq times\n",
    "        # This removes rare typos that confuse the model\n",
    "        for word, count in self.word_counts.items():\n",
    "            if count >= self.min_freq:\n",
    "                self.vocab[word] = len(self.vocab)\n",
    "                \n",
    "        print(f\"Vocabulary built: {len(self.vocab)} unique tokens.\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self.tokenizer(text)\n",
    "        # Convert to IDs\n",
    "        ids = [self.vocab.get(t, self.vocab[\"<UNK>\"]) for t in tokens]\n",
    "        \n",
    "        # Truncate or Pad\n",
    "        if len(ids) > self.max_length:\n",
    "            ids = ids[:self.max_length]\n",
    "        else:\n",
    "            ids = ids + [self.vocab[\"<PAD>\"]] * (self.max_length - len(ids))\n",
    "            \n",
    "        return ids\n",
    "\n",
    "    def prepare_dataset(self, csv_path):\n",
    "        print(f\"Reading {csv_path}...\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        all_texts = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # PASS 1: Collect all text to build vocab\n",
    "        temp_data = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            context = str(row['context'])\n",
    "            question = str(row['question'])\n",
    "            label = int(row['label'])\n",
    "            \n",
    "            try:\n",
    "                options = ast.literal_eval(row['answers'])\n",
    "            except:\n",
    "                options = row['answers']\n",
    "            \n",
    "            # Group the 4 options\n",
    "            q_group = []\n",
    "            for option in options:\n",
    "                # Combined Format: \"Context <SEP> Question Option\"\n",
    "                # We use a special string marker \" ||| \" as a separator\n",
    "                full_text = f\"{context} ||| {question} {option}\"\n",
    "                q_group.append(full_text)\n",
    "                all_texts.append(full_text)\n",
    "                \n",
    "            temp_data.append((q_group, label))\n",
    "            \n",
    "        # Build the vocab from the collected text\n",
    "        self.build_vocab(all_texts)\n",
    "        \n",
    "        # PASS 2: Encode Data\n",
    "        print(\"Encoding Data...\")\n",
    "        tensor_inputs = []\n",
    "        tensor_labels = []\n",
    "        \n",
    "        for q_group, label in temp_data:\n",
    "            encoded_group = [self.encode(text) for text in q_group]\n",
    "            tensor_inputs.append(encoded_group)\n",
    "            tensor_labels.append(label)\n",
    "            \n",
    "        # Convert to Tensors\n",
    "        # Shape: [Num_Questions, 4, Seq_Len]\n",
    "        X = torch.tensor(tensor_inputs, dtype=torch.long)\n",
    "        y = torch.tensor(tensor_labels, dtype=torch.long)\n",
    "        \n",
    "        return TensorDataset(X, y), len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustReasoningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, hidden_dim=100, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 2. BiLSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # 3. Classifier\n",
    "        # Input: Hidden*2 (for BiLSTM) * 2 (for Max+Avg Pooling)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 * 2, 64) \n",
    "        self.classifier = nn.Linear(64, 1) # Final score\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [Batch * 4, Seq_Len]\n",
    "        \n",
    "        # Embed\n",
    "        emb = self.dropout(self.embedding(x))\n",
    "        \n",
    "        # LSTM\n",
    "        # output shape: [Batch, Seq, Hidden*2]\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        \n",
    "        # --- POOLING STRATEGY ---\n",
    "        # Instead of complex Attention, we use Max and Mean pooling\n",
    "        \n",
    "        # 1. Max Pool: Take the maximum value across the sequence dimension\n",
    "        # (Finds the most \"active\" features)\n",
    "        max_pool, _ = torch.max(lstm_out, dim=1) \n",
    "        \n",
    "        # 2. Mean Pool: Average across sequence\n",
    "        mean_pool = torch.mean(lstm_out, dim=1)\n",
    "        \n",
    "        # Concatenate: [Batch, Hidden*4]\n",
    "        combined = torch.cat((max_pool, mean_pool), dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        features = F.relu(self.fc(combined))\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd981bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SETTINGS ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CSV_PATH = r\"aml-2025-read-between-the-lines/train.csv\"\n",
    "\n",
    "# --- PREPARE DATA ---\n",
    "print(\"--- STARTING PIPELINE ---\")\n",
    "pipeline = CustomPipeline(max_length=150, min_freq=2)\n",
    "full_dataset, vocab_size = pipeline.prepare_dataset(CSV_PATH)\n",
    "\n",
    "# --- SPLIT DATA ---\n",
    "val_size = 800\n",
    "train_size = len(full_dataset) - val_size\n",
    "train_subset, val_subset = random_split(\n",
    "    full_dataset, [train_size, val_size], \n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- INITIALIZE MODEL ---\n",
    "print(f\"Initializing Model with Vocab Size: {vocab_size}\")\n",
    "model = RobustReasoningModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=128,    # Increased slightly\n",
    "    hidden_dim=128,   # Increased slightly\n",
    "    dropout=0.4       # Increased dropout for regularization\n",
    ").to(DEVICE)\n",
    "\n",
    "# --- OPTIMIZER ---\n",
    "# Adam is usually safer than Adadelta for scratch training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "EPOCHS = 15\n",
    "\n",
    "print(\"\\n--- STARTING TRAINING ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        # Flatten input: [Batch, 4, Seq] -> [Batch*4, Seq]\n",
    "        b_size, n_opts, seq_len = inputs.shape\n",
    "        flat_inputs = inputs.view(-1, seq_len)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(flat_inputs) # [Batch*4, 1]\n",
    "        \n",
    "        # Reshape for Loss: [Batch, 4]\n",
    "        logits = logits.view(b_size, n_opts)\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    # --- VALIDATION ---\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            b_size, n_opts, seq_len = inputs.shape\n",
    "            flat_inputs = inputs.view(-1, seq_len)\n",
    "            \n",
    "            logits = model(flat_inputs)\n",
    "            logits = logits.view(b_size, n_opts)\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "            \n",
    "    train_acc = train_correct / train_total\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:02d} | Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
