{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import ast\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, Subset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm \n",
    "import itertools\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(hidden_dim))\n",
    "\n",
    "    def forward(self, H):\n",
    "        \"\"\"\n",
    "        H: Output of BiLSTM [Batch, Seq_Len, Hidden_Dim]\n",
    "        \"\"\"\n",
    "        M = torch.tanh(H) \n",
    "        \n",
    "        scores = torch.matmul(M, self.w)  \n",
    "        alpha = F.softmax(scores, dim=1)  \n",
    "        \n",
    "        r = torch.sum(H * alpha.unsqueeze(-1), dim=1) \n",
    "        \n",
    "        h_star = torch.tanh(r)\n",
    "        \n",
    "        return h_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ee64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttBiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.dropout_emb = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout_lstm = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.attention = AttentionLayer(hidden_dim * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        emb = self.dropout_emb(self.embedding(x))\n",
    "        \n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        lstm_out = self.dropout_lstm(lstm_out)\n",
    "        \n",
    "        sentence_vector = self.attention(lstm_out)\n",
    "        \n",
    "        return sentence_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c291e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReasoningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, hidden_dim=100, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = AttBiLSTMEncoder(vocab_size, embed_dim, hidden_dim, dropout)\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim * 2 * 2, 1) \n",
    "\n",
    "    def forward(self, ctx_input, hyp_input):\n",
    "        \"\"\"\n",
    "        Input Shapes: [Batch, 4_Options, Seq_Len]\n",
    "        \"\"\"\n",
    "        batch_size, num_options, seq_len = ctx_input.shape\n",
    "        \n",
    "        flat_ctx = ctx_input.view(-1, seq_len)\n",
    "        flat_hyp = hyp_input.view(-1, seq_len)\n",
    "        \n",
    "        vec_ctx = self.encoder(flat_ctx)\n",
    "        vec_hyp = self.encoder(flat_hyp)\n",
    "        \n",
    "        combined = torch.cat((vec_ctx, vec_hyp), dim=1) \n",
    "        \n",
    "        logits = self.classifier(combined)\n",
    "        \n",
    "        return logits.view(batch_size, num_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085969cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TextPipeline:\n",
    "    def __init__(self, max_length=128):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def prepare_dataset(self, csv_path):\n",
    "        print(f\"Reading {csv_path}...\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        all_ctx = []\n",
    "        all_hyp = []\n",
    "        all_labels = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            context = row['context']\n",
    "            question = row['question']\n",
    "            label = row['label'] \n",
    "            \n",
    "            try:\n",
    "                options = ast.literal_eval(row['answers'])\n",
    "            except:\n",
    "                options = row['answers'] \n",
    "            \n",
    "            \n",
    "            q_ctx = []\n",
    "            q_hyp = []\n",
    "            \n",
    "            for option in options:\n",
    "                q_ctx.append(context)\n",
    "                q_hyp.append(f\"{question} {option}\")\n",
    "                \n",
    "            all_ctx.append(q_ctx)  \n",
    "            all_hyp.append(q_hyp)   \n",
    "            all_labels.append(label)\n",
    "\n",
    "        flat_ctx = [item for sublist in all_ctx for item in sublist]\n",
    "        flat_hyp = [item for sublist in all_hyp for item in sublist]\n",
    "        \n",
    "        print(\"Tokenizing... (This may take a moment)\")\n",
    "        encoded_ctx = self.tokenizer(flat_ctx, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")['input_ids']\n",
    "        encoded_hyp = self.tokenizer(flat_hyp, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")['input_ids']\n",
    "        \n",
    "        num_q = len(df)\n",
    "        tensor_ctx = encoded_ctx.view(num_q, 4, -1)\n",
    "        tensor_hyp = encoded_hyp.view(num_q, 4, -1)\n",
    "        tensor_lbl = torch.tensor(all_labels)\n",
    "        \n",
    "        return TensorDataset(tensor_ctx, tensor_hyp, tensor_lbl), self.tokenizer.vocab_size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CSV_PATH = r\"aml-2025-read-between-the-lines/train.csv\" \n",
    "\n",
    "print(\"Initializing Pipeline...\")\n",
    "pipeline = TextPipeline() \n",
    "full_dataset, vocab_size = pipeline.prepare_dataset(CSV_PATH)\n",
    "\n",
    "print(f\"Vocab Size: {vocab_size}\")\n",
    "print(f\"Dataset Size: {len(full_dataset)}\")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'batch_size': [32],        \n",
    "    'hidden_dim': [64, 100],      \n",
    "    'dropout': [0.3, 0.5],         \n",
    "    'lr': [1.0],                   \n",
    "    'epochs': [10]                  \n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid.items())\n",
    "grid_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "print(f\"Total configurations to test: {len(grid_combinations)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_evaluate(config, dataset, vocab_size):\n",
    "    print(f\"\\nTesting Config: {config}\")\n",
    "    \n",
    "    val_size = 800\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_subset, val_subset = random_split(\n",
    "        dataset, [train_size, val_size], \n",
    "        generator=torch.Generator().manual_seed(42) \n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "\n",
    "    model = ReasoningModel(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=100, \n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "   \n",
    "    optimizer = torch.optim.Adadelta(\n",
    "        model.parameters(), \n",
    "        lr=config['lr'], \n",
    "        weight_decay=1e-5 \n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        for ctx, hyp, lbl in train_loader:\n",
    "            ctx, hyp, lbl = ctx.to(DEVICE), hyp.to(DEVICE), lbl.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(ctx, hyp)\n",
    "            loss = criterion(logits, lbl)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for ctx, hyp, lbl in val_loader:\n",
    "                ctx, hyp, lbl = ctx.to(DEVICE), hyp.to(DEVICE), lbl.to(DEVICE)\n",
    "                logits = model(ctx, hyp)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += (preds == lbl).sum().item()\n",
    "                total += lbl.size(0)\n",
    "        \n",
    "        val_acc = correct / total\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "    print(f\" -> Best Val Acc: {best_val_acc:.4f}\")\n",
    "    return best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b370edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "for config in grid_combinations:\n",
    "    try:\n",
    "        acc = train_evaluate(config, full_dataset, vocab_size)\n",
    "        results.append((acc, config))\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Skipping config {config} due to OOM or Error: {e}\")\n",
    "\n",
    "# --- REPORT RESULTS ---\n",
    "results.sort(key=lambda x: x[0], reverse=True) \n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"GRID SEARCH RESULTS\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Top Performer: {results[0][0]*100:.2f}% accuracy\")\n",
    "best_config = results[0][1] # Grab the dictionary of the best params\n",
    "print(f\"Best Parameters: {best_config}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- RETRAIN FINAL MODEL (CRITICAL FIX) ---\n",
    "# We must re-create the model using the best parameters to save the weights\n",
    "print(\"\\nRetraining final model with best parameters...\")\n",
    "\n",
    "# 1. Setup Data again (Standard Split)\n",
    "val_size = 800\n",
    "train_size = len(full_dataset) - val_size\n",
    "train_subset, val_subset = random_split(\n",
    "    full_dataset, [train_size, val_size], \n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "train_loader = DataLoader(train_subset, batch_size=best_config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=best_config['batch_size'], shuffle=False)\n",
    "\n",
    "# 2. Re-Initialize Model with WINNING Hyperparameters\n",
    "final_model = ReasoningModel(\n",
    "    vocab_size=vocab_size, \n",
    "    embed_dim=100, \n",
    "    hidden_dim=best_config['hidden_dim'], \n",
    "    dropout=best_config['dropout']\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "final_optimizer = torch.optim.Adadelta(\n",
    "    final_model.parameters(), \n",
    "    lr=best_config['lr'], \n",
    "    weight_decay=1e-5 \n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "FINAL_EPOCHS = 10 \n",
    "\n",
    "for epoch in range(FINAL_EPOCHS):\n",
    "    final_model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress = tqdm(train_loader, desc=f\"Final Train Epoch {epoch+1}\")\n",
    "    \n",
    "    for ctx, hyp, lbl in progress:\n",
    "        ctx, hyp, lbl = ctx.to(DEVICE), hyp.to(DEVICE), lbl.to(DEVICE)\n",
    "        \n",
    "        final_optimizer.zero_grad()\n",
    "        logits = final_model(ctx, hyp)\n",
    "        loss = criterion(logits, lbl)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(final_model.parameters(), 5.0)\n",
    "        final_optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress.set_postfix({'loss': loss.item()})\n",
    "\n",
    "SAVE_PATH = \"best_att_bilstm_model.pth\"\n",
    "\n",
    "checkpoint = {\n",
    "    'epoch': FINAL_EPOCHS,\n",
    "    'model_state_dict': final_model.state_dict(),       \n",
    "    'optimizer_state_dict': final_optimizer.state_dict(), \n",
    "    'config': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'embed_dim': 100,\n",
    "        'hidden_dim': best_config['hidden_dim'],\n",
    "        'dropout': best_config['dropout'],\n",
    "        'batch_size': best_config['batch_size']\n",
    "    },\n",
    "    'best_val_accuracy': results[0][0]\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, SAVE_PATH)\n",
    "print(f\"Final model trained and saved to {SAVE_PATH} successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
